import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from pathlib import Path
import xarray as xr
import sys

# Ensure the output encoding is set to UTF-8
sys.stdout.reconfigure(encoding='utf-8')

# Define the base path for the .nc files
base_path = Path('nc_soubory/')
files = sorted(list(base_path.glob('*.nc')))
files = files[2558:2578]  # Select specific files

# Load the data from the .nc files
data = xr.open_mfdataset(files, combine='nested', concat_dim='id_measurement')

data.load()

all_Z = []
all_T = []
all_raw_TS = []

# Loop through each measurement and collect Z, T, and raw_TS
for i in range(data.dims['id_measurement']):  # Loop through each measurement
    data_point = data.isel(id_measurement=i)
    Z = data_point.Z.data
    all_Z.append(Z)
    T = data_point.time.data
    all_T.append(T)
    
    raw_TS = data_point.raw_TS
    tmp = [raw_TS.sel(poly_ch_id=2).values, raw_TS.sel(poly_ch_id=3).values, raw_TS.sel(poly_ch_id=4).values]
    all_raw_TS.append(np.concatenate(tmp))  # Concatenate channel data

# Prepare input data X by stacking Z, T, and raw_TS
X = np.column_stack((all_Z, all_T, all_raw_TS))

# Handle NaN values in the input data
X2 = X.copy()
for i in range(X2.shape[1]):
    column = X2[:, i]
    nan_indices = np.where(np.isnan(column))[0]
    
    # Iterate over each NaN and replace with nearest valid value
    for nan_idx in nan_indices:
        prev_valid = np.nan
        next_valid = np.nan
        
        for j in range(nan_idx - 1, -1, -1):
            if not np.isnan(column[j]):
                prev_valid = column[j]
                break
        
        for j in range(nan_idx + 1, len(column)):
            if not np.isnan(column[j]):
                next_valid = column[j]
                break
        
        if not np.isnan(prev_valid) and not np.isnan(next_valid):
            if nan_idx - j < j - nan_idx:
                column[nan_idx] = prev_valid
            else:
                column[nan_idx] = next_valid
        elif not np.isnan(prev_valid):
            column[nan_idx] = prev_valid
        elif not np.isnan(next_valid):
            column[nan_idx] = next_valid
        X[:, i] = column        

# Assign the cleaned column data back to X
X = X2

# Normalize the data
scaler = StandardScaler()
X = scaler.fit_transform(X)
X.reshape(-1, 1)

# The target variable (Te) is still required as output (labels)
y = data.Te.data[:len(all_Z)]  # Te corresponds to Z and has the same number of entries

# Handle NaN values in y
def handle_nan_values(array):
    for i in range(array.shape[0]):
        if np.isnan(array[i]):
            prev_valid = np.nan
            next_valid = np.nan
            
            for j in range(i - 1, -1, -1):
                if not np.isnan(array[j]):
                    prev_valid = array[j]
                    break
            
            for j in range(i + 1, len(array)):
                if not np.isnan(array[j]):
                    next_valid = array[j]
                    break
            
            if not np.isnan(prev_valid) and not np.isnan(next_valid):
                if i - j < j - i:
                    array[i] = prev_valid
                else:
                    array[i] = next_valid
            elif not np.isnan(prev_valid):
                array[i] = prev_valid
            elif not np.isnan(next_valid):
                array[i] = next_valid

# Handle NaN values in y
handle_nan_values(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the neural network model with Dropout
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)
])

# Compile the model
model.compile(optimizer='adamw', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=30, batch_size=16, validation_split=0.3, verbose=1)

# Plot the loss function over epochs
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Tréninková ztráta')
plt.plot(history.history['val_loss'], label='Validační ztráta')
plt.title("Ztrátová funkce v jednotlivých epochách")
plt.xlabel("Počet epoch")
plt.ylabel("Ztráta (MSE)")
plt.legend()
plt.show()
